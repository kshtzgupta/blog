{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "2021-01-09-Image-Classification-using-MLP-in-PyTorch.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "_Ua8HQiCiKmM"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xm4cLmLiKmR"
      },
      "source": [
        "# MLP in PyTorch using nn.Sequential\n",
        "> In this post, we will implement a MLP in Pytorch using both Functional API and Sequential API to classify MNIST digits\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [jupyter]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pem1XeuiKmR"
      },
      "source": [
        "Let's first import the standard libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9tCMhskiKmS",
        "outputId": "f1ce7ec0-f43d-43ad-b8b9-113604ab0819"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa37bbb5b58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXTtp2LwiZWJ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1E8Qi2diaPA"
      },
      "source": [
        "We will use the MNIST dataset which is a handwritten digits dataset. It contains 60000 training and 10000 testing grayscale 28x28 images from 10 classes (0-9 digits):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXTgzkc5ijGj"
      },
      "source": [
        "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w2_Mnist.png\" width=650>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZWaQbzKElcA"
      },
      "source": [
        "Before defining the dataloader we need to think of batch size. We set training batch size to 64. When you are using a GPU, the maximum batch size is dictated by the memory on the GPU. We’ll use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn’t need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McbI4s5ziKmT"
      },
      "source": [
        "# Training set\r\n",
        "train_dataset = datasets.MNIST('./data', \r\n",
        "                               train=True, \r\n",
        "                               download=True, \r\n",
        "                               transform=transforms.ToTensor())\r\n",
        "\r\n",
        "# Validation dataset\r\n",
        "validation_dataset = datasets.MNIST('./data', \r\n",
        "                                    train=False, \r\n",
        "                                    transform=transforms.ToTensor())\r\n",
        "\r\n",
        "# Batch size : How many images are used to calculate the gradient\r\n",
        "batch_size = 64\r\n",
        "\r\n",
        "# Train DataLoader \r\n",
        "train_loader = DataLoader(dataset=train_dataset, \r\n",
        "                          batch_size=batch_size, \r\n",
        "                          shuffle=True)\r\n",
        "# Validation DataLoader \r\n",
        "val_loader = DataLoader(dataset=validation_dataset, \r\n",
        "                               batch_size=2*batch_size, \r\n",
        "                               shuffle=False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMRqC9p-i_-w"
      },
      "source": [
        "# Create the MLP\r\n",
        "\r\n",
        "Here we define the multi layer perceptron. It has 2 hidden layers with 512 units. Also note that the input layer has 28x28 nodes which is the size of the flattened data. Given below is the schematic diagram of the network.\r\n",
        "\r\n",
        "<img src=\"https://www.learnopencv.com/wp-content/uploads/2017/10/mlp-mnist-schematic.jpg\" width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfJFsUgqjxgH"
      },
      "source": [
        "## Using Functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzjEtzyQjxUc"
      },
      "source": [
        "class functionalMLP(nn.Module):\r\n",
        "    def __init__(self, n_input, n_hid1, n_hid2, n_output):\r\n",
        "        super().__init__()\r\n",
        "        self.hidden1 = nn.Linear(n_input, n_hid1)\r\n",
        "        self.hidden2 = nn.Linear(n_hid1, n_hid2)\r\n",
        "        self.output = nn.Linear(n_hid2, n_output)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # x shape is [batch_size, 1, 28, 28]\r\n",
        "        batch_size = x.shape[0]\r\n",
        "        x = x.view(batch_size, -1)\r\n",
        "        # x shape is [batch_size, 28*28]\r\n",
        "        x = self.relu(self.hidden1(x))\r\n",
        "        x = self.relu(self.hidden2(x))\r\n",
        "        x = self.output(x)\r\n",
        "        return x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAEy2oMEoq5y"
      },
      "source": [
        "model_functional = functionalMLP(n_input=28*28, n_hid1=512, n_hid2=512, n_output=10)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrWwkNMi771f"
      },
      "source": [
        "### Loss function\r\n",
        "\r\n",
        "Since this is classification problem we can use the Cross Entropy Loss available in PyTorch as [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX2CbTK3pkqD"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gA4y0wq95Om"
      },
      "source": [
        "### Optimizer\r\n",
        "\r\n",
        "We define the optimizer as SGD using learning rate of 1e-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DGfZCR1B1eg"
      },
      "source": [
        "def get_opt(model, lr=0.001):\r\n",
        "    opt = optim.SGD(model.parameters(), lr=lr)\r\n",
        "    return opt"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x70tjjoQqH9-"
      },
      "source": [
        "opt = get_opt(model_functional)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIMf-eDe-Xjb"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQxB6on0tFqf"
      },
      "source": [
        "We will define a loss_batch function that encapsulates the backpropogation process for training and loss calculation for validation. This function will also calculate accuracy for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVV9HU0Asm36"
      },
      "source": [
        "def loss_batch(logits, y_batch, loss_func, opt=None):\r\n",
        "    # calculate loss\r\n",
        "    loss = loss_func(logits, y_batch)\r\n",
        "    # calculate accuracy\r\n",
        "    preds = logits.argmax(dim=1)\r\n",
        "    acc = (preds == y_batch).sum()\r\n",
        "\r\n",
        "    # only if we are training then perform the weights update\r\n",
        "    if opt is not None:\r\n",
        "        loss.backward()\r\n",
        "        opt.step()\r\n",
        "        opt.zero_grad()\r\n",
        "    \r\n",
        "    return loss.item(), acc.item()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLDrwe6DtbSX"
      },
      "source": [
        "Next we define a fit function that can handle both training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwaxjpnlpfzv"
      },
      "source": [
        "def fit(epochs, train_loader, val_loader, model, loss_func, opt):\r\n",
        "    for ep in range(epochs):\r\n",
        "        # Training\r\n",
        "        train_losses = []\r\n",
        "        train_n_correct = 0\r\n",
        "        model.train()\r\n",
        "        for x_batch, y_batch in train_loader:\r\n",
        "            logits = model(x_batch)\r\n",
        "            loss, acc = loss_batch(logits, y_batch, loss_func, opt)\r\n",
        "            train_losses.append(loss)\r\n",
        "            train_n_correct += acc\r\n",
        "        train_loss = np.sum(train_losses)/len(train_loader)\r\n",
        "        train_acc = train_n_correct/len(train_loader.dataset)\r\n",
        "        \r\n",
        "        # Validation\r\n",
        "        model.eval()\r\n",
        "        val_losses = []\r\n",
        "        val_n_correct = 0\r\n",
        "        with torch.no_grad():\r\n",
        "            for x_batch, y_batch in val_loader:\r\n",
        "                logits = model(x_batch)\r\n",
        "                loss, acc = loss_batch(logits, y_batch, loss_func)\r\n",
        "                val_losses.append(loss)\r\n",
        "                val_n_correct += acc\r\n",
        "            val_loss = np.sum(val_losses)/len(val_loader)\r\n",
        "            val_acc = val_n_correct/len(val_loader.dataset)\r\n",
        "\r\n",
        "        print(f\"Epoch {ep}:, TrainLoss: {train_loss:.2f}, TrainAcc: {train_acc:.2f}, ValLoss: {val_loss:.2f}, ValAcc: {val_acc:.2f} \")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlJpu8H4-aOL"
      },
      "source": [
        "Now we are ready to begin our model training! We will train for 5 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJUx4IS27yGn",
        "outputId": "3c758ab8-e15a-4f94-e914-ae82bbdc616d"
      },
      "source": [
        "epochs=5\r\n",
        "fit(epochs, train_loader, val_loader, model_functional, loss_func, opt)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:, TrainLoss: 2.28, TrainAcc: 0.32, ValLoss: 2.25, ValAcc: 0.55 \n",
            "Epoch 1:, TrainLoss: 2.22, TrainAcc: 0.63, ValLoss: 2.18, ValAcc: 0.68 \n",
            "Epoch 2:, TrainLoss: 2.14, TrainAcc: 0.69, ValLoss: 2.07, ValAcc: 0.71 \n",
            "Epoch 3:, TrainLoss: 1.98, TrainAcc: 0.71, ValLoss: 1.86, ValAcc: 0.73 \n",
            "Epoch 4:, TrainLoss: 1.73, TrainAcc: 0.73, ValLoss: 1.55, ValAcc: 0.75 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAlvdkaDxvsq"
      },
      "source": [
        "## Using Sequential API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNXt-8r2jWZZ"
      },
      "source": [
        "nn.Sequential is a handy class we can use to simplify our code. A Sequential object runs each of the modules contained within it, in a sequential manner. This is a simpler way of writing our MLP.\r\n",
        "\r\n",
        "To take advantage of this, we need to be able to easily define a custom layer from a given function. For instance, PyTorch doesn’t have a view layer, and we need to create one for our network. Lambda will create a layer that we can then use when defining a network with Sequential."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccw9lSSBjC8U"
      },
      "source": [
        "class Lambda(nn.Module):\r\n",
        "    def __init__(self, func):\r\n",
        "        super().__init__()\r\n",
        "        self.func = func\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.func(x)\r\n",
        "\r\n",
        "def preprocess(x):\r\n",
        "    return x.view(x.shape[0], -1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmN2ThVk6g4a"
      },
      "source": [
        "Now we create the model using nn.Sequential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF4LRTRd6crv"
      },
      "source": [
        "model_sequential = nn.Sequential(\r\n",
        "    Lambda(preprocess),\r\n",
        "    nn.Linear(784, 512),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear(512, 512),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear(512, 10))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt803AXD7CBs"
      },
      "source": [
        "And that's it! This model defined using nn.Sequential is equivalent to the one we defined using Functional API. We re-use the fit and loss_batch functions defined above to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAc0ZaEf7uF8",
        "outputId": "e10248c8-8adb-42d7-d1a5-8b8b2a1b92eb"
      },
      "source": [
        "epochs=5\r\n",
        "opt = get_opt(model_sequential)\r\n",
        "fit(epochs, train_loader, val_loader, model_sequential, loss_func, opt)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:, TrainLoss: 2.28, TrainAcc: 0.25, ValLoss: 2.26, ValAcc: 0.39 \n",
            "Epoch 1:, TrainLoss: 2.23, TrainAcc: 0.47, ValLoss: 2.20, ValAcc: 0.56 \n",
            "Epoch 2:, TrainLoss: 2.16, TrainAcc: 0.58, ValLoss: 2.10, ValAcc: 0.63 \n",
            "Epoch 3:, TrainLoss: 2.03, TrainAcc: 0.64, ValLoss: 1.93, ValAcc: 0.69 \n",
            "Epoch 4:, TrainLoss: 1.82, TrainAcc: 0.69, ValLoss: 1.66, ValAcc: 0.72 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS-_bdSU7r8i"
      },
      "source": [
        "**Hope you enjoyed reading! Please leave questions or feedback in the comments!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFuxf0obK-pd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}